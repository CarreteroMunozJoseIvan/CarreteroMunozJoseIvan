# ---------- OPTIONAL: install best matcher (comment if locked env) ----------
# !pip install rapidfuzz unidecode

import re
import pandas as pd
from datetime import timedelta

# ======================== SETTINGS ========================
DAYFIRST   = True   # True -> dd/mm/yyyy; False -> mm/dd/yyyy
TOL_DAYS   = 0      # allow ± days around inception date (use 2–3 if small off-by errors)
THRESHOLD  = 90     # fuzzy score to accept name match (85–92 typical)

# If you need to load files here, uncomment and set paths:
# df1 = pd.read_excel('Escape - Dead Accounts.xlsx')
# df2 = pd.read_excel('Everview_Data.xlsx')
# df3 = pd.read_excel('UK&I_DATA_TRACKER.xlsx')

# ==================== FUZZY ENGINE PICK ====================
try:
    from rapidfuzz import process, fuzz
    _USE_RAPIDFUZZ = True
except Exception:
    from difflib import SequenceMatcher
    _USE_RAPIDFUZZ = False

try:
    from unidecode import unidecode
except Exception:
    def unidecode(x): return x

# ===================== CLEANING HELPERS ====================
LEGAL_SUFFIXES  = r'\b(SA|S\.A\.|SAS|SL|S\.L\.|PLC|LLC|LTD|PTE|BV|GMBH|AG|SPA|S\.P\.A\.|SRL|S\.R\.L\.|INC|CO|COMPANY|CORP|CORPORATION|LIMITED|PTY|NV|AB|ASA|KFT|OY|AS|LLP)\b'
GENERIC_WORDS   = r'\b(THE|GROUP|HOLDING|HOLDINGS|INDUSTRIES|INDUSTRY|SERVICES?|RESOURCES?)\b'
BROKER_SUFFIXES = r'\b(LTD|LIMITED|LLP|LLC|BV|GMBH|INC|CO|COMPANY|CORP|BROKING|BROKERS?)\b'

def clean_text(s: str) -> str:
    s = '' if pd.isna(s) else str(s)
    s = unidecode(s).upper()
    s = re.sub(r'[\(\)\[\]\{\}]', ' ', s)
    s = re.sub(r'\d+', ' ', s)
    s = re.sub(r'[^A-Z0-9 &/-]+', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def clean_insured(x: str) -> str:
    x = clean_text(x)
    x = re.sub(LEGAL_SUFFIXES, ' ', x)
    x = re.sub(GENERIC_WORDS, ' ', x)
    x = re.sub(r'\s+', ' ', x).strip()
    return x

def clean_broker(x: str) -> str:
    x = clean_text(x)
    x = re.sub(BROKER_SUFFIXES, ' ', x)
    x = x.replace('ARTHUR J GALLAGHER', 'GALLAGHER').replace('A J GALLAGHER', 'GALLAGHER')
    x = x.replace('WILLIS TOWERS WATSON', 'WTW').replace('WTW LIMITED', 'WTW')
    x = x.replace('PRICE FORBES & PARTNERS', 'PRICE FORBES')
    x = re.sub(r'\s+', ' ', x).strip()
    return x

def parse_date_col(s):
    return pd.to_datetime(s, errors='coerce', dayfirst=DAYFIRST)

def fuzzy_best(query, choices_series):
    if choices_series.empty or not isinstance(query, str) or not query:
        return 0, None
    if _USE_RAPIDFUZZ:
        val, score, idx = process.extractOne(query, choices_series, scorer=fuzz.token_set_ratio) or (None,0,None)
        return score, idx
    else:
        best_score, best_idx = 0.0, None
        for idx, val in choices_series.items():
            score = SequenceMatcher(None, query, val).ratio()*100
            if score > best_score:
                best_score, best_idx = score, idx
        return best_score, best_idx

# ================== STANDARDIZE YOUR DATA ==================
# df1 columns (from your screenshot):
# ['Primary Insured Name','Effective Date','Expiration Date']
df1_std = df1.rename(columns={
    'Primary Insured Name':'insured',
    'Effective Date':'inception_date',
    'Expiration Date':'expiration_date'
}).copy()

# df2 columns:
# ['INSURED','OWNER','ASSIGNEE','BROKER','INCEPTION_DATE','EXPIRY_DATE','RISK_PROCESSING_STATUS']
df2_std = df2.rename(columns={
    'INSURED':'insured',
    'INCEPTION_DATE':'inception_date',
    'EXPIRY_DATE':'expiration_date',
    'BROKER':'broker'
}).copy()

# df3 columns:
# ['Insured','Inception Date','Live Status','Broker']
df3_std = df3.rename(columns={
    'Insured':'insured',
    'Inception Date':'inception_date',
    'Broker':'broker'
}).copy()

for d in (df1_std, df2_std, df3_std):
    d['inception_date'] = parse_date_col(d['inception_date'])
    if 'expiration_date' in d.columns:
        d['expiration_date'] = parse_date_col(d['expiration_date'])
    if 'broker' not in d.columns:
        d['broker'] = None
    d['insured_clean'] = d['insured'].map(clean_insured)
    d['broker_clean']  = d['broker'].map(clean_broker) if 'broker' in d.columns else None

# ============= MATCHER (FIXED DATE INDEX ISSUE) =============
def match_on_date_and_name(left, right, right_cols_keep,
                           left_date_col, right_date_col,
                           tol_days=TOL_DAYS, threshold=THRESHOLD):
    """
    Robust matcher that:
      - accepts explicit date column names on each side,
      - converts to datetime,
      - builds a date index on 'right' safely,
      - does fuzzy match on 'insured_clean' within the date window.
    Returns: left + matched right columns + score.
    """
    # Normalize headers (trim -> lower)
    left  = left.rename(columns=lambda c: c.strip())
    right = right.rename(columns=lambda c: c.strip())
    left.columns  = [c.lower() for c in left.columns]
    right.columns = [c.lower() for c in right.columns]
    right_cols_keep = [c.strip().lower() for c in right_cols_keep]
    left_date_col  = left_date_col.strip().lower()
    right_date_col = right_date_col.strip().lower()

    # Validate columns
    if left_date_col not in left.columns:
        raise KeyError(f"'{left_date_col}' not in left.columns: {list(left.columns)}")
    if right_date_col not in right.columns:
        raise KeyError(f"'{right_date_col}' not in right.columns: {list(right.columns)}")

    # Ensure dates are datetime
    left[left_date_col]   = pd.to_datetime(left[left_date_col],  errors='coerce', dayfirst=DAYFIRST)
    right[right_date_col] = pd.to_datetime(right[right_date_col], errors='coerce', dayfirst=DAYFIRST)

    # Build safe date index on the right
    right_idx = right.set_index(right_date_col, drop=False)

    out = []
    for _, r in left.iterrows():
        d = r[left_date_col]
        if pd.isna(d):
            out.append({**r.to_dict(), 'match_score': 0})
            continue

        if tol_days > 0:
            lo, hi = d - pd.Timedelta(days=tol_days), d + pd.Timedelta(days=tol_days)
            candidates = right_idx[(right_idx[right_date_col] >= lo) & (right_idx[right_date_col] <= hi)]
        else:
            candidates = right_idx[right_idx[right_date_col] == d]

        if candidates.empty:
            out.append({**r.to_dict(), 'match_score': 0})
            continue

        score, idx = fuzzy_best(r['insured_clean'], candidates['insured_clean'])
        if idx is None or score < threshold:
            out.append({**r.to_dict(), 'match_score': score})
            continue

        payload = candidates.loc[idx, right_cols_keep].to_dict()
        out.append({**r.to_dict(), 'match_score': score, **payload})

    return pd.DataFrame(out)


# =============== RUN MATCHES (df1 as pivot) ===============
left_base = df1_std[['insured','inception_date','expiration_date','insured_clean']].copy()

# df1 -> df2
keep2 = ['insured','insured_clean','inception_date','expiration_date',
         'broker','broker_clean','ASSIGNEE','OWNER','RISK_PROCESSING_STATUS']
m12 = match_on_date_and_name(
    left=left_base.copy(),
    right=df2_std.copy(),
    right_cols_keep=[c for c in keep2 if c.lower() in [x.lower() for x in df2_std.columns]],
    left_date_col='inception_date',
    right_date_col='inception_date',
    tol_days=TOL_DAYS, threshold=THRESHOLD
).rename(columns={
    'insured':'df1_insured','insured_clean':'df1_name_clean',
    'inception_date':'df1_inception','expiration_date':'df1_expiry',
    'match_score':'df2_score',
    # matched right cols (rename for clarity)
    'insured_y':'df2_insured','insured_clean_y':'df2_name_clean',
    'inception_date_y':'df2_inception','expiration_date_y':'df2_expiry',
    'broker':'df2_broker','broker_clean':'df2_broker_clean',
    'assignee':'df2_assignee','owner':'df2_owner',
    'risk_processing_status':'df2_risk_status'
})

# df1 -> df3
keep3 = ['insured','insured_clean','inception_date','broker','broker_clean','Live Status']
m13 = match_on_date_and_name(
    left=left_base.copy(),
    right=df3_std.copy(),
    right_cols_keep=[c for c in keep3 if c.lower() in [x.lower() for x in df3_std.columns]],
    left_date_col='inception_date',
    right_date_col='inception_date',
    tol_days=TOL_DAYS, threshold=THRESHOLD
).rename(columns={
    'insured':'df1_insured','insured_clean':'df1_name_clean',
    'inception_date':'df1_inception','expiration_date':'df1_expiry',
    'match_score':'df3_score',
    'insured_y':'df3_insured','insured_clean_y':'df3_name_clean',
    'inception_date_y':'df3_inception',
    'broker':'df3_broker','broker_clean':'df3_broker_clean',
    'live status':'df3_live_status'
})

# Ensure unique column names after the generic rename
for col in ['insured','insured_clean','inception_date','expiration_date']:
    m12.rename(columns={f'{col}_y': f'df2_{col}',
                        f'{col}': f'df1_{col}' if f'df1_{col}' in m12.columns else col}, inplace=True)
    m13.rename(columns={f'{col}_y': f'df3_{col}',
                        f'{col}': f'df1_{col}' if f'df1_{col}' in m13.columns else col}, inplace=True)

# Keys to merge both match tables
keys = ['df1_insured','df1_name_clean','df1_inception','df1_expiry']
res = pd.merge(
    m12[[*keys, 'df2_score','df2_insured','df2_name_clean','df2_inception','df2_expiry',
         'df2_broker','df2_broker_clean','df2_assignee','df2_owner','df2_risk_status']],
    m13[[*keys, 'df3_score','df3_insured','df3_name_clean','df3_inception',
         'df3_broker','df3_broker_clean','df3_live_status']],
    on=keys, how='outer'
)

# ===================== QUALITY FLAGS ======================
res['match_df2'] = res['df2_score'].fillna(0).ge(THRESHOLD)
res['match_df3'] = res['df3_score'].fillna(0).ge(THRESHOLD)
res['triple_match'] = res['match_df2'] & res['match_df3']

def brokers_equal(b1, b2):
    if (not b1) and (not b2): return True
    if (not b1) or (not b2):  return False
    if b1 == b2: return True
    if _USE_RAPIDFUZZ:
        return fuzz.token_set_ratio(b1, b2) >= 92
    else:
        return SequenceMatcher(None, b1, b2).ratio() >= 0.92

res['df2_df3_broker_same'] = res.apply(
    lambda r: brokers_equal(r.get('df2_broker_clean'), r.get('df3_broker_clean')), axis=1
)

# Optional: expiry agreement between df1 and df2
if 'df2_expiry' in res.columns:
    res['expiry_same_1_2'] = (
        pd.to_datetime(res['df1_expiry']).dt.date ==
        pd.to_datetime(res['df2_expiry']).dt.date
    )

# ===================== QUICK VIEWS ========================
triple = res[res['triple_match']].sort_values(['df2_score','df3_score'], ascending=False)
needs_review = res[(~res['match_df2']) | (~res['match_df3']) | (~res['df2_df3_broker_same'].fillna(True))]

print('df1 rows:', len(df1_std))
print('Triple matches:', len(triple))
print('Need review:', len(needs_review))
res.head(20)

# ===================== OPTIONAL EXPORT ====================
# with pd.ExcelWriter('
