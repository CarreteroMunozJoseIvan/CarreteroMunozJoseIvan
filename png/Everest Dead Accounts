# --- Optional installs (comment out if your env is locked) ---
# !pip install rapidfuzz unidecode

import re
import pandas as pd
from datetime import timedelta

# ======= CONFIG (tune these) =======
TOL_DAYS   = 0      # allow ± days around inception date (use 2–3 if small off-by errors)
THRESHOLD  = 90     # fuzzy name score to accept match (lower to 85/80 if needed)
DAYFIRST   = True   # True -> dd/mm/yyyy; False -> mm/dd/yyyy

# If you want to load files here, uncomment and set paths:
# df1 = pd.read_excel('Escape - Dead Accounts.xlsx')
# df2 = pd.read_excel('Everview_Data.xlsx')
# df3 = pd.read_excel('UK&I_DATA_TRACKER.xlsx')

# ======= Fuzzy engine selection =======
try:
    from rapidfuzz import process, fuzz
    _USE_RAPIDFUZZ = True
except Exception:
    from difflib import SequenceMatcher
    _USE_RAPIDFUZZ = False

try:
    from unidecode import unidecode
except Exception:
    def unidecode(x): return x

# ======= Cleaning helpers =======
LEGAL_SUFFIXES = r'\b(SA|S\.A\.|SAS|SL|S\.L\.|PLC|LLC|LTD|PTE|BV|GMBH|AG|SPA|S\.P\.A\.|SRL|S\.R\.L\.|INC|CO|COMPANY|CORP|CORPORATION|LIMITED|PTY|NV|AB|ASA|KFT|OY|AS|LLP)\b'
GENERIC_WORDS  = r'\b(THE|GROUP|HOLDING|HOLDINGS|INDUSTRIES|INDUSTRY|SERVICES?|RESOURCES?)\b'
BROKER_SUFFIXES = r'\b(LTD|LIMITED|LLP|LLC|BV|GMBH|INC|CO|COMPANY|CORP|BROKING|BROKERS?)\b'

def clean_text(s: str) -> str:
    s = '' if pd.isna(s) else str(s)
    s = unidecode(s).upper()
    s = re.sub(r'[\(\)\[\]\{\}]', ' ', s)
    s = re.sub(r'\d+', ' ', s)
    s = re.sub(r'[^A-Z0-9 &/-]+', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def clean_insured(x: str) -> str:
    x = clean_text(x)
    x = re.sub(LEGAL_SUFFIXES, ' ', x)
    x = re.sub(GENERIC_WORDS, ' ', x)
    x = re.sub(r'\s+', ' ', x).strip()
    return x

def clean_broker(x: str) -> str:
    x = clean_text(x)
    x = re.sub(BROKER_SUFFIXES, ' ', x)
    # Common normalisations
    x = x.replace('ARTHUR J GALLAGHER', 'GALLAGHER').replace('A J GALLAGHER', 'GALLAGHER')
    x = x.replace('WILLIS TOWERS WATSON', 'WTW').replace('WTW LIMITED', 'WTW')
    x = x.replace('PRICE FORBES & PARTNERS', 'PRICE FORBES')
    x = re.sub(r'\s+', ' ', x).strip()
    return x

def parse_date_col(s):  # robust parse
    return pd.to_datetime(s, errors='coerce', dayfirst=DAYFIRST)

def fuzzy_best(query, choices_series):
    if choices_series.empty or not isinstance(query, str) or not query:
        return 0, None
    if _USE_RAPIDFUZZ:
        res = process.extractOne(query, choices_series, scorer=fuzz.token_set_ratio)
        if res is None: return 0, None
        val, score, idx = res
        return score, idx
    else:
        best_score, best_idx = 0.0, None
        for idx, val in choices_series.items():
            score = SequenceMatcher(None, query, val).ratio() * 100
            if score > best_score:
                best_score, best_idx = score, idx
        return best_score, best_idx

# ======= Rename/standardise columns from your screenshots =======
# df1: ['Primary Insured Name','Effective Date','Expiration Date']
df1_std = df1.rename(columns={
    'Primary Insured Name':'insured',
    'Effective Date':'inception_date',
    'Expiration Date':'expiration_date'
}).copy()

# df2: ['INSURED','OWNER','ASSIGNEE','BROKER','INCEPTION_DATE','EXPIRY_DATE','RISK_PROCESSING_STATUS']
df2_std = df2.rename(columns={
    'INSURED':'insured',
    'INCEPTION_DATE':'inception_date',
    'EXPIRY_DATE':'expiration_date',
    'BROKER':'broker'
}).copy()

# df3: ['Insured','Inception Date','Live Status','Broker']
df3_std = df3.rename(columns={
    'Insured':'insured',
    'Inception Date':'inception_date',
    'Broker':'broker'
}).copy()

# Dates + clean fields
for d in (df1_std, df2_std, df3_std):
    d['inception_date'] = parse_date_col(d['inception_date'])
    if 'broker' not in d.columns: d['broker'] = None
    d['insured_clean'] = d['insured'].map(clean_insured)
    d['broker_clean']  = d['broker'].map(clean_broker) if 'broker' in d.columns else None

# ======= Matching function (robust date-col handling) =======
def match_on_date_and_name(left, right, right_cols_keep,
                           left_date_col='inception_date',
                           right_date_col='inception_date',
                           tol_days=TOL_DAYS, threshold=THRESHOLD):
    # Trim + lower headers
    left  = left.rename(columns={c: c.strip() for c in left.columns})
    right = right.rename(columns={c: c.strip() for c in right.columns})
    left.columns  = [c.lower() for c in left.columns]
    right.columns = [c.lower() for c in right.columns]
    right_cols_keep = [c.strip().lower() for c in right_cols_keep]
    left_date_col  = left_date_col.lower()
    right_date_col = right_date_col.lower()

    # Validate & coerce dates
    if left_date_col not in left.columns:
        raise KeyError(f"'{left_date_col}' not in left.columns: {list(left.columns)}")
    if right_date_col not in right.columns:
        raise KeyError(f"'{right_date_col}' not in right.columns: {list(right.columns)}")

    left[left_date_col]   = pd.to_datetime(left[left_date_col],  errors='coerce', dayfirst=DAYFIRST)
    right[right_date_col] = pd.to_datetime(right[right_date_col], errors='coerce', dayfirst=DAYFIRST)

    right_idx = right.set_index(right_date_col)
    rows = []

    for _, r in left.iterrows():
        d = r[left_date_col]
        if pd.isna(d):
            rows.append({**r.to_dict(), 'match_idx': None, 'match_score': 0})
            continue

        if tol_days > 0:
            mask = (right_idx.index >= d - pd.Timedelta(days=tol_days)) & \
                   (right_idx.index <= d + pd.Timedelta(days=tol_days))
            candidates = right_idx.loc[mask]
        else:
            candidates = right_idx.loc[right_idx.index == d]

        if candidates.empty:
            rows.append({**r.to_dict(), 'match_idx': None, 'match_score': 0})
            continue

        score, idx = fuzzy_best(r['insured_clean'], candidates['insured_clean'])
        if idx is None or score < threshold:
            rows.append({**r.to_dict(), 'match_idx': None, 'match_score': score})
            continue

        mrow = candidates.loc[idx]
        payload = mrow[right_cols_keep].to_dict()
        rows.append({**r.to_dict(), 'match_idx': idx, 'match_score': score, **payload})
    return pd.DataFrame(rows)

# ======= Build left sides (df1 pivot) =======
left_base = df1_std[['insured','inception_date','expiration_date']].copy()
left_base['insured_clean'] = left_base['insured'].map(clean_insured)

# df1 -> df2
keep2 = ['insured','insured_clean','inception_date','expiration_date','broker','broker_clean',
         'ASSIGNEE','OWNER','RISK_PROCESSING_STATUS']
m12 = match_on_date_and_name(
    left=left_base.copy(),
    right=df2_std.copy(),
    right_cols_keep=[c for c in keep2 if c.lower() in [x.lower() for x in df2_std.columns] or c in ['insured_clean','broker_clean']],
    left_date_col='inception_date',
    right_date_col='inception_date',
    tol_days=TOL_DAYS, threshold=THRESHOLD
).rename(columns={
    'insured':'df1_insured','inception_date':'df1_inception','expiration_date':'df1_expiry',
    'insured_clean':'df1_name_clean',
    'match_score':'df2_score',
    'insured_x':'_drop'
})

# df1 -> df3
keep3 = ['insured','insured_clean','inception_date','broker','broker_clean','Live Status']
m13 = match_on_date_and_name(
    left=left_base.copy(),
    right=df3_std.copy(),
    right_cols_keep=[c for c in keep3 if c.lower() in [x.lower() for x in df3_std.columns] or c in ['insured_clean','broker_clean']],
    left_date_col='inception_date',
    right_date_col='inception_date',
    tol_days=TOL_DAYS, threshold=THRESHOLD
).rename(columns={
    'insured':'df1_insured','inception_date':'df1_inception','expiration_date':'df1_expiry',
    'insured_clean':'df1_name_clean',
    'match_score':'df3_score'
})

# Rename matched columns for clarity
m12 = m12.rename(columns={
    'insured':'df2_insured','insured_clean':'df2_name_clean',
    'inception_date':'df2_inception','expiration_date':'df2_expiry',
    'broker':'df2_broker','broker_clean':'df2_broker_clean',
    'ASSIGNEE':'df2_assignee','OWNER':'df2_owner','RISK_PROCESSING_STATUS':'df2_risk_status'
})
m13 = m13.rename(columns={
    'insured':'df3_insured','insured_clean':'df3_name_clean',
    'inception_date':'df3_inception',
    'broker':'df3_broker','broker_clean':'df3_broker_clean',
    'Live Status':'df3_live_status'
})

# ======= Merge the two match tables (df1 as key) =======
keys = ['df1_insured','df1_name_clean','df1_inception','df1_expiry']
res = pd.merge(
    m12[keys + ['df2_insured','df2_name_clean','df2_inception','df2_expiry','df2_score',
                'df2_broker','df2_broker_clean','df2_assignee','df2_owner','df2_risk_status']],
    m13[keys + ['df3_insured','df3_name_clean','df3_inception','df3_score',
                'df3_broker','df3_broker_clean','df3_live_status']],
    on=keys, how='outer'
)

# ======= Flags (quality + broker) =======
res['match_df2'] = res['df2_score'].fillna(0).ge(THRESHOLD)
res['match_df3'] = res['df3_score'].fillna(0).ge(THRESHOLD)
res['triple_match'] = res['match_df2'] & res['match_df3']

def broker_equal(b1, b2):
    if (not b1) and (not b2): return True
    if (not b1) or (not b2):  return False
    if b1 == b2: return True
    if _USE_RAPIDFUZZ:
        return fuzz.token_set_ratio(b1, b2) >= 92
    else:
        return SequenceMatcher(None, b1, b2).ratio() >= 0.92

# df2 vs df3 broker
res['df2_df3_broker_same'] = res.apply(lambda r: broker_equal(r.get('df2_broker_clean'), r.get('df3_broker_clean')), axis=1)

# (Optional) Expiry agreement df1 vs df2
if 'df2_expiry' in res.columns:
    res['expiry_same_1_2'] = (pd.to_datetime(res['df1_expiry']).dt.date ==
                               pd.to_datetime(res['df2_expiry']).dt.date)

# Useful views
triple = res[res['triple_match']].sort_values(['df2_score','df3_score'], ascending=False)
needs_review = res[(~res['match_df2']) | (~res['match_df3']) | (~res['df2_df3_broker_same'].fillna(True))]

print('df1 rows:', len(df1_std))
print('Triple matches:', len(triple))
print('Need review:', len(needs_review))

# Uncomment to export
# with pd.ExcelWriter('matching_report.xlsx', engine='xlsxwriter') as xw:
#     res.to_excel(xw, index=False, sheet_name='all')
#     triple.to_excel(xw, index=False, sheet_name='triple_matches')
#     needs_review.to_excel(xw, index=False, sheet_name='needs_review')

# Peek
res.head(20)

